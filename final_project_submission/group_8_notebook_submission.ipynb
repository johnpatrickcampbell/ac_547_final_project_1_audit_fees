{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# --- AC 547 Final Project Group 8 Notebook ---",
   "id": "467be366a02ffad5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Import packages",
   "id": "aade99f35e637bf9"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import statsmodels.formula.api as smf\n",
    "from matplotlib.ticker import FuncFormatter"
   ],
   "id": "ca3b7a5220847b88",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Load the datasets and verify they are loaded correctly.",
   "id": "544fc853deaa3c68"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "audit_fees = pd.read_csv('audit_fees_from_wrds.csv', sep=',')\n",
    "cams = pd.read_csv('cams_from_wrds.csv', sep=',')\n",
    "audit_opinions = pd.read_csv('audit_opinions_wrds.csv', sep=',')\n",
    "\n",
    "print(\"---Audit Fees Data---\")\n",
    "print(audit_fees.head())\n",
    "print(\"---CAMs Data---\")\n",
    "print(cams.head())\n",
    "print(\"---Audit Opinions Data---\")\n",
    "print(audit_opinions.head())"
   ],
   "id": "c69d617d385e83da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# --- Step 1: Filter Audit Fees for Fiscal Years 2020-2025 and Big 4 Firms ---",
   "id": "844c4cadba358b82"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Filter audit_fees for Fiscal Years 2020-2025",
   "id": "2e41e3fe501dab5d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "audit_fees_filtered = audit_fees[\n",
    "    (audit_fees['FISCAL_YEAR'] >= 2020) & (audit_fees['FISCAL_YEAR'] <= 2025)\n",
    "]"
   ],
   "id": "4664035e4ed595c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Filter audit_fees_filtered for Big 4 Firms",
   "id": "424144501165c3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the keywords for the Big 4 firms\n",
    "big4_keywords = ['PricewaterhouseCoopers', 'Ernst & Young', 'Deloitte', 'KPMG']\n",
    "\n",
    "# Create a filter (mask) that checks if the auditor name contains any of these keywords\n",
    "mask_big4 = audit_fees_filtered['CURR_AUD_NAME'].str.contains('|'.join(big4_keywords), na=False)\n",
    "\n",
    "# Apply the filter to create a new dataframe\n",
    "audit_fees_filtered_big_4 = audit_fees_filtered[mask_big4].copy()\n",
    "\n",
    "# Check the results\n",
    "print(f\"Original Row Count: {len(audit_fees_filtered)}\")\n",
    "print(f\"Big 4 Row Count: {len(audit_fees_filtered_big_4)}\")\n",
    "print(\"\\n--- Auditor Counts ---\")\n",
    "print(audit_fees_filtered_big_4['CURR_AUD_NAME'].value_counts())"
   ],
   "id": "2d72562f61f72bed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# --- Step 2: Merge Opinions into CAMs to get the Fiscal Year ---",
   "id": "28343c2489cf1085"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We only need the key and the year from the audit_opinions file\n",
    "We create a subset so we don't duplicate columns we don't need"
   ],
   "id": "c13d81ac47724c76"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "opinions_subset = audit_opinions[['AUDIT_OP_KEY', 'FISCAL_YEAR_OF_OP']]\n",
    "\n",
    "# Merge them into the CAMs dataframe\n",
    "# left_on points to the key in 'cams', right_on points to the key in 'opinions'\n",
    "cams_with_year = pd.merge(\n",
    "    cams,\n",
    "    opinions_subset,\n",
    "    left_on='AUDIT_OPINION_FKEY',\n",
    "    right_on='AUDIT_OP_KEY',\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Verify the merge: Check if 'FISCAL_YEAR_OF_OP' now exists in the CAMs data\n",
    "print(\"--- CAMs with Year Added ---\")\n",
    "print(cams_with_year[['CRITICAL_AUDIT_MATTER_KEY', 'FISCAL_YEAR_OF_OP']].head())"
   ],
   "id": "18fce9034213ad0a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Check how many CAMs failed to find a year (Missing values)\n",
    "missing_years = cams_with_year['FISCAL_YEAR_OF_OP'].isna().sum()\n",
    "print(f\"Rows with missing years: {missing_years}\")\n",
    "\n",
    "# Drop rows where the year is missing (we can't use them)\n",
    "cams_with_year = cams_with_year.dropna(subset=['FISCAL_YEAR_OF_OP'])\n",
    "\n",
    "# Convert the year column to Integer (removes the .0)\n",
    "cams_with_year['FISCAL_YEAR_OF_OP'] = cams_with_year['FISCAL_YEAR_OF_OP'].astype(int)\n",
    "\n",
    "# Check the result\n",
    "print(\"--- Fixed Years ---\")\n",
    "print(cams_with_year[['CRITICAL_AUDIT_MATTER_KEY', 'FISCAL_YEAR_OF_OP']].head())"
   ],
   "id": "1a67090941c63589",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Pivot the CAMs Table\n",
    "\n",
    "cam_types = pd.pivot_table(\n",
    "    cams_with_year,\n",
    "    index=['COMPANY_FKEY', 'FISCAL_YEAR_OF_OP'],\n",
    "    columns='TOPIC_NAME',\n",
    "    aggfunc='size',\n",
    "    fill_value=0\n",
    ").reset_index()\n",
    "\n",
    "# Clean up column names\n",
    "# We'll add a prefix \"CAM_\" to make them easy to identify later\n",
    "cam_types.columns.name = None  # Remove the index name\n",
    "new_columns = ['COMPANY_FKEY', 'FISCAL_YEAR_OF_OP'] + ['CAM_' + col.replace(' ', '_') for col in cam_types.columns[2:]]\n",
    "cam_types.columns = new_columns\n",
    "\n",
    "\n",
    "# We still want the TOTAL count (including the rare ones we filtered out)\n",
    "# So let's merge this back with our original total count\n",
    "total_counts = cams_with_year.groupby(['COMPANY_FKEY', 'FISCAL_YEAR_OF_OP']).size().reset_index(name='NUM_CAMS')\n",
    "\n",
    "# Merge detailed types with total counts\n",
    "final_cam_data = pd.merge(total_counts, cam_types, on=['COMPANY_FKEY', 'FISCAL_YEAR_OF_OP'], how='left')\n",
    "\n",
    "print(final_cam_data.head())"
   ],
   "id": "69fa675075689c1e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# --- Step 3: Merge Audit Fees with CAM Counts ---",
   "id": "4df2a49f0162084e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Merge audit_fees_filtered_big_4 (left) with final_cam_data (right)\n",
    "# Left Key: COMPANY_FKEY, FISCAL_YEAR\n",
    "# Right Key: COMPANY_FKEY, FISCAL_YEAR_OF_OP\n",
    "df_merged = pd.merge(\n",
    "    audit_fees_filtered_big_4,\n",
    "    final_cam_data,\n",
    "    left_on=['COMPANY_FKEY', 'FISCAL_YEAR'],\n",
    "    right_on=['COMPANY_FKEY', 'FISCAL_YEAR_OF_OP'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Check the result\n",
    "print(\"--- Merged Data with CAM Types ---\")\n",
    "# Show specific columns to verify\n",
    "cols_to_show = ['COMPANY_FKEY', 'FISCAL_YEAR', 'AUDIT_FEES', 'NUM_CAMS']\n",
    "print(df_merged[cols_to_show].head())"
   ],
   "id": "a808b39bac12a397",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# --- Step 4: Clean Merged Data ---",
   "id": "448bd6be836ee42"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fill NaN values in NUM_CAMS with 0 (since they didn't match the CAM file)\n",
    "df_merged['NUM_CAMS'] = df_merged['NUM_CAMS'].fillna(0)\n",
    "\n",
    "# Fill NaN values in the specific CAM topic columns with 0 as well\n",
    "# (We get the list of CAM columns from the merged dataframe)\n",
    "cam_cols = [col for col in df_merged.columns if 'CAM_' in col]\n",
    "df_merged[cam_cols] = df_merged[cam_cols].fillna(0)"
   ],
   "id": "6f77010982c0d600",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Rename Columns\n",
    "\n",
    "df_merged_renamed = df_merged.rename(columns={\n",
    "    'MATCHFY_BALSH_BOOK_VAL': 'BOOK_VAL',\n",
    "    'MATCHFY_BALSH_ASSETS': 'TOTAL_ASSETS',\n",
    "    'MATCHFY_INCMST_NETINC_TTM': 'NET_INCOME_TTM',\n",
    "    'MATCHFY_INCOME_STMT_REVENUE': 'REVENUE',\n",
    "    'SIC_CODE_FKEY': 'SIC_CODE'\n",
    "})"
   ],
   "id": "e3b711a68c36caa1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Handle Missing Data\n",
    "# Rationale: We must drop any row that is missing a variable\n",
    "# we intend to use in our model.\n",
    "print(f\"Original row count: {len(df_merged_renamed)}\")\n",
    "\n",
    "# We define our list of \"must-have\" columns for the regression\n",
    "required_cols = ['AUDIT_FEES', 'TOTAL_ASSETS', 'BOOK_VAL',\n",
    "                 'NET_INCOME_TTM', 'SIC_CODE', 'NUM_CAMS']\n"
   ],
   "id": "d5d414bc09610bf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# .dropna() removes any row with 'NaN' in this subset\n",
    "df_clean = df_merged_renamed.dropna(subset=required_cols)\n",
    "\n",
    "print(f\"Row count after dropping NaNs: {len(df_clean)}\")"
   ],
   "id": "da2c17683106dde4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- INVESTIGATION CODE ---\n",
    "\n",
    "# 1. Check where the missing values are coming from\n",
    "print(\"--- Missing Values by Column (Before Drop) ---\")\n",
    "missing_counts = df_merged_renamed[required_cols].isna().sum()\n",
    "print(missing_counts)\n",
    "\n",
    "# 2. Check specifically how many rows have missing CAM counts\n",
    "missing_cams = df_merged_renamed['NUM_CAMS'].isna().sum()\n",
    "print(f\"\\nRows with missing NUM_CAMS (likely 0 CAMs): {missing_cams}\")"
   ],
   "id": "cd816a1699ac21ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 4.2: Handle Non-Positive Values ---\n",
    "# Rationale: We cannot take the log of zero or a negative number.\n",
    "# Financial data (like 'AUDIT_FEES' and 'TOTAL_ASSETS') must be\n",
    "# positive. We'll filter for this. 'book_value' can be negative,\n",
    "# so we'll be careful.\n",
    "\n",
    "df_clean = df_clean[df_clean['AUDIT_FEES'] > 0]\n",
    "df_clean = df_clean[df_clean['TOTAL_ASSETS'] > 0]\n",
    "\n",
    "# For book_value, we will only log it if it's positive.\n",
    "# For net_income, we'll create a separate 'is_loss' flag.\n",
    "\n",
    "print(f\"Row count after ensuring 'fees' and 'assets' > 0: {len(df_clean)}\")"
   ],
   "id": "818053b2ae0a41af",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "### Part 4.3: Feature Engineering (Log Transforms)\n",
    "# Rationale: This is a key part of our proposal. We log these\n",
    "# variables to reduce skewness and model the relationships\n",
    "# in proportional terms (elasticity).\n",
    "\n",
    "# Log transform fees and assets\n",
    "df_clean['LOG_AUDIT_FEES'] = np.log(df_clean['AUDIT_FEES'])\n",
    "\n",
    "# --- CORRECTED LINE BELOW ---\n",
    "# My original code had a typo. This is the correct line.\n",
    "df_clean['LOG_TOTAL_ASSETS'] = np.log(df_clean['TOTAL_ASSETS'])\n",
    "\n",
    "# Log transform book_value only where it's positive\n",
    "# We create 'LOG_BOOK_VAL' and fill negatives/zeros with NaN\n",
    "# This line is *expected* to produce a RuntimeWarning, which is fine.\n",
    "df_clean['LOG_BOOK_VAL'] = np.where(\n",
    "    df_clean['BOOK_VAL'] > 0,\n",
    "    np.log(df_clean['BOOK_VAL']),\n",
    "    np.nan\n",
    ")\n",
    "\n",
    "# For cam_count, we use log(x + 1) to handle the '0' values.\n",
    "# This is a standard transformation for count data.\n",
    "df_clean['LOG_NUM_CAMS'] = np.log(df_clean['NUM_CAMS'] + 1)"
   ],
   "id": "26d960449e58ce23",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 4.4: Feature Engineering (Other Variables) ---\n",
    "# Rationale: We create a simple 'IS_LOSS' dummy variable for\n",
    "# profitability and simplify 'SIC_CODE' to a 2-digit industry.\n",
    "\n",
    "# Create a 1/0 dummy variable for 'is_loss'\n",
    "df_clean['IS_LOSS'] = (df_clean['NET_INCOME_TTM'] < 0).astype(int)\n",
    "\n",
    "# Convert SIC code to a 2-digit industry group\n",
    "# .str.slice(0, 2) takes the first two characters\n",
    "df_clean['SIC_CODE'] = df_clean['SIC_CODE'].astype(str).str.slice(0, 2)\n",
    "\n",
    "print(\"Feature engineering complete.\")"
   ],
   "id": "91ac0afb63be773e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- 4.5: Final Inspection of Cleaned Data ---\n",
    "print(\"\\n--- Final Clean DataFrame Head ---\")\n",
    "print(df_clean.head())\n",
    "\n",
    "print(\"\\n--- Final Clean DataFrame Info ---\")\n",
    "df_clean.info()\n",
    "\n",
    "print(\"\\n--- Final Clean DataFrame Statistics (describe) ---\")\n",
    "# .describe() is a *mandatory* step. It shows us the\n",
    "# mean, median, min, and max of our new variables.\n",
    "print(df_clean.describe())"
   ],
   "id": "d4ab9260b8a6d3ac",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# --- Step 5: Descriptive Statistics ---",
   "id": "a9c960f1c6f2f01c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Define the variables we want to summarize\n",
    "# We include both the raw values (for interpretation) and logged values (for the model)\n",
    "desc_vars = [\n",
    "    'AUDIT_FEES', 'LOG_AUDIT_FEES',\n",
    "    'TOTAL_ASSETS', 'LOG_TOTAL_ASSETS',\n",
    "    'BOOK_VAL',     # We included this in our \"High Quality\" set\n",
    "    'LOG_BOOK_VAL', 'NET_INCOME_TTM',\n",
    "    'NUM_CAMS', 'LOG_NUM_CAMS',\n",
    "    'IS_LOSS'\n",
    "]"
   ],
   "id": "1ff45eaa94bcedc5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 2. Generate the statistics\n",
    "# We use .describe() and then select specific metrics\n",
    "desc_stats = df_clean[desc_vars].describe().T[['count', 'mean', 'std', 'min', '50%', 'max']]"
   ],
   "id": "2762b584277dced7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 3. Rename '50%' to 'Median' for professional presentation\n",
    "desc_stats = desc_stats.rename(columns={'50%': 'median'})"
   ],
   "id": "f3cb1b28348741ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4. Format the output to make it readable (2 decimal places)\n",
    "print(\"--- Descriptive Statistics (N = 13,986) ---\")\n",
    "print(desc_stats.apply(lambda x: x.map('{:,.2f}'.format)))"
   ],
   "id": "726288d94981a809",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Optional: Check the \"frequency\" of specific CAM topics if you are interested\n",
    "print(\"\\n--- Frequency of Top 5 CAM Topics ---\")\n",
    "cam_cols = [c for c in df_clean.columns if 'CAM_' in c]\n",
    "print(df_clean[cam_cols].mean().sort_values(ascending=False).head(5))"
   ],
   "id": "cd4ef1d99c1a252a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# --- Step 7: OLS Regression Analysis ---",
   "id": "7ad2de3ebaf007ce"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define the Regression Formula\n",
    "# We predict Log Audit Fees using:\n",
    "# - Size (LOG_TOTAL_ASSETS)\n",
    "# - Complexity (LOG_NUM_CAMS)\n",
    "# - Risk (IS_LOSS)\n",
    "# - Valuation/Growth (LOG_BOOK_VAL)\n",
    "# - Industry Fixed Effects (C(SIC_CODE))\n",
    "formula = 'LOG_AUDIT_FEES ~ LOG_TOTAL_ASSETS + LOG_NUM_CAMS + IS_LOSS + LOG_BOOK_VAL + C(SIC_CODE)'"
   ],
   "id": "1616e2b5bc37a76e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Fit the Model\n",
    "# statsmodels will automatically drop the few rows where LOG_BOOK_VAL is NaN (negative equity)\n",
    "model = smf.ols(formula=formula, data=df_clean)\n",
    "results = model.fit()\n",
    "\n",
    "print(results.summary())"
   ],
   "id": "7e76a8c5fb336b35",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# --- Step 8: Visualizations ---",
   "id": "ba0a1ed7ef5353e2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Identify CAM columns (they start with 'CAM_')\n",
    "cam_cols = [col for col in df_clean.columns if 'CAM_' in col]\n",
    "\n",
    "# 2. Sum the counts for each topic and sort descending\n",
    "cam_frequencies = df_clean[cam_cols].sum().sort_values(ascending=False)\n",
    "\n",
    "# 3. Clean up the names for the chart (remove 'CAM_' and underscores)\n",
    "cam_frequencies.index = [idx.replace('CAM_', '').replace('_', ' ') for idx in cam_frequencies.index]\n",
    "\n",
    "# 4. Plot the Top 10\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=cam_frequencies.head(10).values, y=cam_frequencies.head(10).index, color='#004225')\n",
    "\n",
    "plt.title('Top 10 Most Frequent Critical Audit Matters (CAMs)', fontsize=14)\n",
    "plt.xlabel('Frequency (Count)', fontsize=12)\n",
    "plt.ylabel('CAM Topic', fontsize=12)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "a67c65128acf0024",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Create a \"CAM Group\" column to make the legend cleaner\n",
    "# We group 4 or more CAMs together since they are rare\n",
    "def bin_cams(x):\n",
    "    if x == 0: return '0 CAMs'\n",
    "    elif x == 1: return '1 CAM'\n",
    "    elif x == 2: return '2 CAMs'\n",
    "    elif x == 3: return '3 CAMs'\n",
    "    else: return '4+ CAMs'\n",
    "\n",
    "df_clean['CAM_Group'] = df_clean['NUM_CAMS'].apply(bin_cams)\n",
    "\n",
    "# 2. Define the order for the legend so it makes sense (0 -> 4+)\n",
    "hue_order = ['0 CAMs', '1 CAM', '2 CAMs', '3 CAMs', '4+ CAMs']\n",
    "\n",
    "# 3. Create the Scatter Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(\n",
    "    data=df_clean,\n",
    "    x='LOG_TOTAL_ASSETS',\n",
    "    y='LOG_AUDIT_FEES',\n",
    "    hue='CAM_Group',       # Color points by group\n",
    "    hue_order=hue_order,   # Use our custom order\n",
    "    alpha=0.6,             # Make points slightly transparent\n",
    "    palette='crest'      # Use a colorblind-friendly palette\n",
    ")\n",
    "\n",
    "# 4. Add Labels and Title\n",
    "plt.title('Audit Fees vs. Company Size by CAM Count', fontsize=14)\n",
    "plt.xlabel('Log Total Assets (Size)', fontsize=12)\n",
    "plt.ylabel('Log Audit Fees', fontsize=12)\n",
    "plt.legend(title='Number of CAMs')\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "9bcfad54ccf6553e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# --- Bar Chart of Median Audit Fees with Names ---\n",
    "\n",
    "# 1. Filter for the Top 10 most common industries\n",
    "top_10_sic = df_clean['SIC_CODE'].value_counts().head(10).index\n",
    "df_top_sic = df_clean[df_clean['SIC_CODE'].isin(top_10_sic)].copy()\n",
    "\n",
    "# 2. Create the \"Name Mapper\"\n",
    "# This dictionary matches the 2-digit code to the Industry Name\n",
    "sic_names = {\n",
    "    '73': 'Business Services',\n",
    "    '28': 'Chemicals & Allied Products',\n",
    "    '36': 'Electronic Equipment & Components',\n",
    "    '35': 'Machinery',\n",
    "    '38': 'Instruments (Medical/Optical)',\n",
    "    '62': 'Security & Commodity Brokers',\n",
    "    '67': 'Holding Offices',\n",
    "    '63': 'Insurance Carriers',\n",
    "    '48': 'Communications',\n",
    "    '13': 'Oil & Gas Extraction',\n",
    "    '50': 'Wholesale Trade - Durable Goods',\n",
    "    '87': 'Engineering/Accounting Services',\n",
    "    '60': 'Depository Institutions',\n",
    "    '49': 'Electric, Gas & Sanitary Services',\n",
    "    '37': 'Transportation Equipment'\n",
    "\n",
    "}\n",
    "\n",
    "# 3. Apply the Names\n",
    "# If a code isn't in our list (rare), it falls back to \"SIC XX\"\n",
    "df_top_sic['Industry_Name'] = df_top_sic['SIC_CODE'].map(sic_names).fillna('SIC ' + df_top_sic['SIC_CODE'])\n",
    "\n",
    "# 4. Define the Sort Order (Most Common -> Least Common)\n",
    "# value_counts() automatically sorts by count descending\n",
    "frequency_order = df_top_sic['Industry_Name'].value_counts().index\n",
    "\n",
    "# Calculate the medians (we don't need to sort this dataframe anymore)\n",
    "industry_medians = df_top_sic.groupby('Industry_Name')['AUDIT_FEES'].median().reset_index()\n",
    "\n",
    "# 5. Create the Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "\n",
    "# Save the plot object to a variable 'ax' so we can manipulate the text\n",
    "ax = sns.barplot(\n",
    "    data=industry_medians,\n",
    "    x='AUDIT_FEES',\n",
    "    y='Industry_Name',\n",
    "    color='#004225',\n",
    "    legend=False,\n",
    "    order=frequency_order # Ensures 'Most Common' is at the top (index 0)\n",
    ")\n",
    "\n",
    "# --- FONT & FORMATTING CUSTOMIZATION ---\n",
    "plt.title('Top 10 Most Common Industries Median Audit Fees', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Median Audit Fees ($ Millions)', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Remove the default vertical Y-label\n",
    "ax.set_ylabel('')\n",
    "\n",
    "# --- ADDING THE SPLIT LABELS ---\n",
    "# transform=ax.transAxes allows us to use relative coordinates (0 to 1)\n",
    "# rather than data coordinates.\n",
    "# (0, 1) is Top-Left. (0, 0) is Bottom-Left.\n",
    "\n",
    "# 1. \"Top 10 Industries\" at the Top Left\n",
    "ax.text(x=-0.1, y=1, s=\"Top 10 Industries (By Frequency)\",\n",
    "        transform=ax.transAxes,\n",
    "        ha='right', va='bottom', fontsize=12, fontweight='bold', color='#333333')\n",
    "\n",
    "# --- Y-AXIS NUMBERING (1 to 10) ---\n",
    "# Create new labels with the rank prepended: \"1. Industry Name\", etc.\n",
    "new_labels = [f\"{i+1}. {name}\" for i, name in enumerate(frequency_order)]\n",
    "ax.set_yticks(range(len(frequency_order)))\n",
    "ax.set_yticklabels(new_labels, fontsize=12)\n",
    "\n",
    "\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# --- THE MAGIC FORMATTER ---\n",
    "# Ensure you have: from matplotlib.ticker import FuncFormatter\n",
    "def millions_formatter(x, pos):\n",
    "    return f'{x / 1e6:.1f}M'\n",
    "\n",
    "plt.gca().xaxis.set_major_formatter(FuncFormatter(millions_formatter))\n",
    "\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "49d6017c6e6d695b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 1. Prepare Data: Calculate Median Fees for ALL industries first\n",
    "median_fees = df_clean.groupby('SIC_CODE')['AUDIT_FEES'].median()\n",
    "\n",
    "# 2. Sort Descending and Take Top 10\n",
    "top_10_highest = median_fees.sort_values(ascending=False).head(10).reset_index()\n",
    "top_10_highest.columns = ['SIC_CODE', 'Median_Fee']\n",
    "\n",
    "# 3. Updated Dictionary for High-Fee Industries\n",
    "sic_names_high = {\n",
    "    '21': 'Tobacco Products',\n",
    "    '29': 'Petroleum Refining',\n",
    "    '63': 'Insurance Carriers',\n",
    "    '89': 'Services, NEC',\n",
    "    '26': 'Paper & Allied Products',\n",
    "    '20': 'Food and Kindred Products',\n",
    "    '75': 'Auto Repair, Services, & Parking',\n",
    "    '51': 'Wholesale Trade - Nondurable Goods',\n",
    "    '39': 'Misc. Manufacturing Industries',\n",
    "    '52': 'Building Mat. & Garden Supplies'\n",
    "}\n",
    "\n",
    "# 4. Apply Names\n",
    "top_10_highest['Industry_Name'] = top_10_highest['SIC_CODE'].map(sic_names_high).fillna('SIC ' + top_10_highest['SIC_CODE'])\n",
    "\n",
    "# 5. Create the Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(\n",
    "    data=top_10_highest,\n",
    "    x='Median_Fee',\n",
    "    y='Industry_Name',\n",
    "    color='#004225',\n",
    "    legend=False,  # Using 'magma' to signify \"hot/expensive\"\n",
    ")\n",
    "\n",
    "# --- FONT & FORMATTING ---\n",
    "plt.title('Top 10 Industries with Highest Median Audit Fees', fontsize=16, fontweight='bold')\n",
    "plt.xlabel('Median Audit Fees ($ Millions)', fontsize=14, fontweight='bold')\n",
    "plt.ylabel('Industry', fontsize=14, fontweight='bold')\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "\n",
    "# Format X-axis to \"6.0M\"\n",
    "def millions_formatter(x, pos):\n",
    "    return f'{x / 1e6:.1f}M'\n",
    "\n",
    "plt.gca().xaxis.set_major_formatter(FuncFormatter(millions_formatter))\n",
    "\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "132f93e9c6ea3993",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
